# 1. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®é›†
```python
# è¯»å–æ•°æ®é›†

df = pd.read_csv('https://drive.google.com/uc?id=1D3Ma2HcEkkt85d5OtXxL5f8MHmrwvuQi&export=download', sep=',')

# Keep only those rows of the data frames with score>10

# åˆæ­¥ç­›é€‰

df_upten = df[df['score'] > 10]

# Make a copy of the dataframe

df_copy = df_upten[['label', 'comment']].copy()

# ç­›é€‰è®­ç»ƒé›†å’Œæµ‹è¯•é›†
length = len(df_copy)

temp = int(length * 0.8)

data_train = df_copy[:temp]

data_test = df_copy[temp:]

```




# 2. Character level tokenization and filtering of smaller/longer sentences

* Tokenize the data by fitting the train data with a vocabulary size of 100, using character-level tokenization instead of whitespace tokenization. è¯æ±‡å¤§å°ä¸º100ï¼Œå­—ç¬¦ç”¨æ ‡è®°è€Œä¸æ˜¯ç©ºç™½

* Preprocess the data, so that the label column is of type int and the comments are represented as vectors got by tokenization.

* Filter tokenized comments that have less than 10 and more than 100 tokens for both the train and test sets.


```python
# Define a tokenizer with vocabulary size of 100 that filters all special characters

data_train.comment = data_train.comment.str.replace("[^a-zA-Z0-9]", '', regex=True).str.lower()

data_test.comment = data_test.comment.str.replace("[^a-zA-Z0-9]", '', regex=True).str.lower()

# å®šä¹‰è‡ªå·±çš„ tokenizer

tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=100, char_level=True, oov_token='UNK')

# Fit the tokenizer on the comments column of the train set

tokenizer.fit_on_texts(data_train['comment'])
word_index = tokenizer.word_index

# Convert the label column of the train and test
label_train = data_train.label.values
label_test = data_test.label.values
# Convert the comments column to sequences
data_train = tokenizer.texts_to_sequences(data_train['comment'])
data_test = tokenizer.texts_to_sequences(data_test['comment'])

# Create a boolean mask to filter out sentences which are less than 10 tokens/ more than 100 tokens.
mask_train = [(10 <= len(seq) <= 100) for seq in data_train]
data_train = [seq for seq, mask in zip(data_train, mask_train) if mask]
label_train = [seq for seq, mask in zip(label_train, mask_train) if mask]

# Similarly create a mask to filter the test dataset
mask_test = [(10 <= len(seq) <= 100) for seq in data_test]
data_test= [seq for seq, mask in zip(data_test, mask_test) if mask]
label_test = [seq for seq, mask in zip(label_test, mask_train) if mask]
```

é‡è¦çš„å‡ ä¸ªå‡½æ•°ï¼š

1. å®šä¹‰è‡ªå·±çš„ tokenizer

```python
tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=100, char_level=True, oov_token='UNK')
```

2. ç”Ÿæˆ tokenizer å¹¶åº”ç”¨

```python
tokenizer.fit_on_texts(data_train['comment'])
data_train = tokenizer.texts_to_sequences(data_train['comment'])
data_test = tokenizer.texts_to_sequences(data_test['comment'])
```


# 3. Tensorflow Datasetåˆ¶ä½œï¼ˆtraining & validation)

1. æ•°æ®é›†è½¬æ¢æˆå±‚æ¬¡ä¸é½ï¼ˆraggedï¼‰çš„å¼ é‡
2. åˆ›å»ºå¼ é‡æµæ•°æ®é›†
3. è®¾ç½®æ‰¹æ¬¡ï¼Œå¯¹æ•°æ®é›†è¿›è¡Œæ··æ´—ï¼ˆshuffleï¼‰å’Œæ‰¹é‡ï¼ˆbatchï¼‰å¤„ç†
4. ç”¨ map å‡½æ•°å¡«å……æ•°æ®
5. ğŸŒŸé¢„å–ï¼ˆprefetchï¼‰æ“ä½œï¼Œå¯¹æ•°æ®æå‰åŠ è½½å‡å°‘ç½‘ç»œç­‰å¾…çš„æ—¶é—´

```python
# Use tensorflow ragged constants to get the ragged version of the train data
ragged_train = tf.ragged.constant(data_train)
label_train = tf.ragged.constant(label_train)

# Convert the train set into a tensorflow dataset
dataset_train = tf.data.Dataset.from_tensor_slices((ragged_train, label_train))

# Shuffle the dataset
buffer_size = len(data_train)
dataset_train = dataset_train.shuffle(buffer_size)


# Batch the dataset into batches of 512
batch_size = 512
dataset_train = dataset_train.batch(batch_size)

# Pad each input batch with 0s
dataset_train = dataset_train.map(lambda x,y: (x.to_tensor(default_value=0, shape=[None, None]), y), num_parallel_calls=5)

# Prefetching
prefetch_buffer_size = tf.data.experimental.AUTOTUNE
dataset_train = dataset_train.prefetch(prefetch_buffer_size)

# Use tensorflow ragged constants to get the ragged version of the test data
ragged_test_data = tf.ragged.constant(data_test)

# Index to word mapping
index_2_word = {index: word for word, index in tokenizer.word_index.items()}


# Reconstruct the sentence using tokens
''.join([index_2_word[i] for i in X_tr[0].numpy() if i!=0])

# Checking sequence length counts across batches
seq_len = []
for X_tr, y_tr in dataset_train.as_numpy_iterator():
seq_len.append(X_tr.shape[1])

# Checking the count of seq lengths
Counter(seq_len)


```

# 4. æ¨¡å‹å®šä¹‰å’Œè®­ç»ƒ

æ„å»º GRU æ¨¡å‹
```python
# Define the model

model2 = Sequential()
model2.add(Embedding(input_dim=10000, output_dim=128))
# CNN layers
model2.add(Conv1D(filters=64, kernel_size=5, activation='relu'))
model2.add(MaxPooling1D(pool_size=4))
# Deep Bidirectional GRU layers
# Assuming you already have a GRU setup, add it here
model2.add(Bidirectional(GRU(64, return_sequences=True)))
model2.add(Bidirectional(GRU(64)))
# Dense layers for final processing and output
# Tailor this to your specific task (e.g., number of units and activation function)
model2.add(Dense(64, activation='relu'))
model2.add(Dropout(0.5))
model2.add(Dense(1, activation='sigmoid')) # Adjust for your specific task

# Compile the model
model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
# Model summary
model2.summary()
```

# 5. Evaluate the modelæ¨¡å‹è¯„ä¼°