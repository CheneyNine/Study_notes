 # Recap: Language Models

è¯­è¨€æ¨¡å‹å°±æ˜¯ç”¨ä¹‹å‰çš„å•è¯æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯
 ![[Pasted image 20231229142448.png]]

## ç»“æ„
4 ä¸ªéƒ¨åˆ†ï¼šè¾“å…¥å¤„ç†ã€embeddingã€ç‰¹å¾æå–ã€ä»»åŠ¡æ‰§è¡Œ
### Input preprocessing

From corpus -> vocabulary
remove Upper letter/emoji/punctuation/grammatical mistakes


### Embedding

This stage transforms the data to be fed to the network. The embedding layer transforms every word into *a fixed length* vector with arbitrary dimension. åµŒå…¥å±‚å°†æ¯ä¸ªå•è¯è½¬æ¢ä¸ºä»»æ„ç»´åº¦çš„å›ºå®šé•¿åº¦å‘é‡ã€‚ One-hot-encoding or word2vec are some examples.

### Feature Extraction

Text->vector-> another vector
another vector stands for a feature

From the sequence of embeddings, we generate a new representation which will be used in the next stage. Its job is to capture as much contextual information as possible.
æ ¹æ® embedding åºåˆ—ï¼Œç”Ÿæˆæ–°çš„è¡¨ç¤ºå‘é‡ï¼Œè·å¾—å°½å¯èƒ½å¤šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

represent depend on the context
### Task

Exampleï¼šPredict
åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨äº¤å‰ç†µæŸå¤±å°†å½’ä¸€åŒ–é¢„æµ‹ä¸çœŸå®å€¼ y æ¯”è¾ƒï¼ˆyä¸€èˆ¬æ˜¯ one hot ç¼–ç çš„ï¼‰
è®­ç»ƒå®Œæ¨¡å‹åå°±å°† Neural Network å›ºå®šå¹¶ä¸æ”¹å˜
![[Pasted image 20231229145724.png]]

# Recap: Neural Networks as Language Models

![[Pasted image 20231229151236.png]]
# ELMo
## Bidirectional two-layer LSTM Language Model
To create the ELMo embeddings we could use an LSTM network.

### Two layer
é‡‡ç”¨åŒå±‚ç»“æ„æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½
![[Pasted image 20231229151410.png]]

ä¸ºäº†è§£å†³ä¼˜åŒ–é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨ [[4. æ·±åº¦å­¦ä¹ #è§£å†³æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜]]  ä¸­çš„æ®‹å·®ç½‘ç»œ residual connection
![[Pasted image 20231229151847.png]]
å…¶ä¸­ ä¸‹ä¸€ä¸ªçŠ¶æ€ hä¼šç”¨ç¬¬ä¸€ä¸ª LSTM æ¨¡å‹çš„è¾“å‡ºç»“æœç»“åˆä¸ŠåŸæœ‰çš„ X_tæ¥è¿›è¡Œè¿ç®—ï¼Œå…¶ä¸­ W è¡¨ç¤ºä¸€ä¸ªçº¿æ€§å˜æ¢æ“ä½œï¼Œç”¨äºè°ƒæ•´ Xt çš„è¾“å…¥
![[Pasted image 20231229152052.png]]
###  bidirectional åŒå‘
ä¹‹æ‰€ä»¥é‡‡ç”¨åŒå‘çš„ LSTMï¼š æ˜¯å› ä¸ºè¯­ä¹‰ä¿¡æ¯ä¸ä»…ä»…æ˜¯ç”±è¿‡å»å†³å®šçš„ï¼Œæœªæ¥çš„ä¿¡æ¯ä¹Ÿå¾ˆé‡è¦ã€‚
The softmax layers for the forward and the backward LSTM are the same. But W is different.
![[Pasted image 20231229152725.png]]
We optimize both networks at the same time. For each word, we optimize both RNNs jointly:

$$
L = -\sum y \log(\hat{y}_{\text{right}}) - \sum y \log(\hat{y}_{\text{left}})
$$

## Character CNN  
### åŸå…ˆçš„é—®é¢˜
ä¼šå‡ºç° Unkownï¼Œä¸”æ²¡æœ‰å…³æ³¨ context
å› æ­¤é‡‡ç”¨ characters ä¼šå¾ˆæ–¹ä¾¿
### å…·ä½“æ“ä½œ
ç”¨ä¸‰ä¸ªå½¢çŠ¶ä¸åŒçš„å·ç§¯æ ¸æ¥æå–ç‰¹å¾
ä¸€ä¸ªå½¢çŠ¶çš„å·ç§¯æ ¸åšnæ¬¡å¤„ç†ï¼Œæ¯æ¬¡å·ç§¯æ ¸ä¸­çš„å…·ä½“æ•°å­—ä¹Ÿä¸ä¸€æ ·
![[Pasted image 20231229162136.png]]
ä¸ºäº†è®©æ¯ä¸ªå•è¯å½¢æˆçš„é•¿åº¦éƒ½æ˜¯ä¸€æ ·çš„ï¼Œæˆ‘ä»¬é‡‡ç”¨æ± åŒ–æ“ä½œ
![[Pasted image 20231229162311.png]]
è¿™æ ·å­ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªèƒ½å¤Ÿä» Words ä¸­æå– information å¹¶ä¸”è¾“å‡ºé•¿åº¦ä¸€è‡´çš„ embedding
ä½†è¿™äº›ä¿¡æ¯ä»…ä¸ CNN æå–åˆ°çš„å±€éƒ¨ç‰¹å¾ç›¸å…³ã€‚æˆ‘ä»¬éœ€è¦ä»¥æŸç§æ–¹å¼ç»“åˆè¿™äº›ä¿¡æ¯ã€‚
#### Highway Networks

>[!quate] ChatGPT
>Highway Networks æ˜¯ä¸€ç§ç¥ç»ç½‘ç»œç»“æ„ï¼Œå®ƒå¼•å…¥äº†ä¸€ç§ç§°ä¸ºâ€œé—¨æ§â€ï¼ˆgating mechanismï¼‰çš„æ¦‚å¿µï¼Œå…è®¸ä¿¡æ¯æ— éšœç¢åœ°ç©¿è¿‡ç½‘ç»œä¸­çš„å¤šä¸ªå±‚ã€‚è¿™ç§ç»“æ„æ˜¯ç”± Srivastava ç­‰äººåœ¨2015å¹´æå‡ºçš„ï¼Œç›®çš„æ˜¯è§£å†³åœ¨è®­ç»ƒæ·±å±‚ç½‘ç»œæ—¶å¸¸è§çš„é—®é¢˜ï¼Œå¦‚æ¢¯åº¦æ¶ˆå¤±å’Œè¡¨ç¤ºç“¶é¢ˆã€‚
>
åœ¨ä¼ ç»Ÿçš„ç¥ç»ç½‘ç»œä¸­ï¼Œæ¯ä¸€å±‚çš„è¾“å‡ºæ˜¯å‰ä¸€å±‚çš„è¾“å‡ºç»è¿‡åŠ æƒã€åç½®å¹¶åº”ç”¨æ¿€æ´»å‡½æ•°åçš„ç»“æœã€‚éšç€å±‚æ•°çš„å¢åŠ ï¼Œä¿¡æ¯å’Œæ¢¯åº¦å¿…é¡»é€šè¿‡æ‰€æœ‰è¿™äº›å˜æ¢ï¼Œè¿™å¯èƒ½å¯¼è‡´è®­ç»ƒå›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨æ¢¯åº¦åŸºäºåå‘ä¼ æ’­ç®—æ³•æ—¶ã€‚
>
ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼ŒHighway Networks å¼•å…¥äº†ä¸¤ä¸ªæ–°çš„æ“ä½œï¼šè½¬æ¢é—¨ï¼ˆtransform gateï¼‰å’Œè½½å…¥é—¨ï¼ˆcarry gateï¼‰ã€‚è¿™äº›é—¨æ§åˆ¶ç€ä¿¡æ¯æ˜¯åº”è¯¥è¢«ä¼ é€’åˆ°ä¸‹ä¸€å±‚ï¼Œè¿˜æ˜¯åº”è¯¥åœ¨å½“å‰å±‚è¿›è¡Œå˜æ¢ã€‚
>
>1. **è½¬æ¢é—¨ï¼ˆTï¼‰**ï¼šå®ƒå†³å®šäº†å¤šå°‘åŸå§‹ä¿¡æ¯è¦ç»è¿‡å˜æ¢ã€‚è¿™ä¸ªé—¨çš„è¾“å‡ºæ˜¯0åˆ°1ä¹‹é—´çš„å€¼ï¼Œä¸å½“å‰å±‚çš„è¾“å…¥ç›¸ä¹˜ï¼Œè¿™ä¸ªä¹˜ç§¯å†³å®šäº†å¤šå°‘è¾“å…¥ä¿¡æ¯ä¼šè¢«å˜æ¢ã€‚
>
>2. **è½½å…¥é—¨ï¼ˆCï¼‰**ï¼šè¿™ä¸ªé—¨ä¸è½¬æ¢é—¨ç›¸å¯¹åº”ï¼Œå®ƒå†³å®šäº†å¤šå°‘ä¿¡æ¯ä¸ç»å˜æ¢ç›´æ¥ä¼ é€’åˆ°ä¸‹ä¸€å±‚ã€‚è½½å…¥é—¨é€šå¸¸è¢«è®¾ç½®ä¸º \(1 - T\)ï¼Œè¿™æ ·å°±ä¿è¯äº†ä¸€éƒ¨åˆ†ä¿¡æ¯æ€»æ˜¯èƒ½å¤Ÿé€šè¿‡ï¼Œå³ä½¿æ˜¯åœ¨æ·±å±‚ç½‘ç»œä¸­ã€‚
>
é€šè¿‡è¿™ç§ç»“æ„ï¼ŒHighway Networks å…è®¸ä¿¡æ¯åœ¨å¿…è¦æ—¶ç»è¿‡å¤šä¸ªå±‚çš„å¤æ‚å˜æ¢ï¼ŒåŒæ—¶ä¹Ÿå…è®¸ä¿¡æ¯ç›´æ¥é€šè¿‡ï¼Œè¿™æ ·æœ‰åŠ©äºä¿æŒæ¢¯åº¦æµå’Œä¿¡æ¯æµï¼Œä»è€Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ï¼Œæé«˜äº†ç½‘ç»œçš„è®­ç»ƒæ•ˆç‡ã€‚
>
æ€»ç»“ä¸€ä¸‹ï¼ŒHighway Networks é€šè¿‡å¼•å…¥é—¨æ§æœºåˆ¶ï¼Œæä¾›äº†ä¸€ç§çµæ´»çš„æ–¹å¼æ¥å¹³è¡¡ä¿¡æ¯åœ¨ç½‘ç»œä¸­çš„å˜æ¢å’Œç›´æ¥ä¼ é€’ï¼Œä½¿å¾—æ·±å±‚ç½‘ç»œçš„è®­ç»ƒå˜å¾—æ›´åŠ å¯è¡Œã€‚

![[Pasted image 20231229163355.png]]
 ![[Pasted image 20231229170820.png]]![[Pasted image 20231229163417.png]]
  C=1-T
  CNN çš„è¾“å‡ºæ˜¯æ¯ä¸ªå•è¯çš„å­—ç¬¦çº§ç‰¹å¾è¡¨ç¤ºã€‚è¿™äº›è¡¨ç¤ºéšåé€šè¿‡ä¸€ä¸ªæˆ–å¤šä¸ª Highway Networks å±‚ï¼ŒHighway å±‚çš„ä½œç”¨æ˜¯é€‰æ‹©æ€§åœ°ä¼ é€’å’Œè½¬æ¢ä¿¡æ¯ã€‚åœ¨ ELMo ä¸­ï¼ŒHighway Networks å…è®¸æŸäº›ä¿¡æ¯ï¼ˆæ¯”å¦‚å­—ç¬¦çº§çš„ç‰¹å¾ï¼‰ä¸ç»å˜åŒ–åœ°ç›´æ¥ä¼ é€’ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥å¯¹ä¿¡æ¯è¿›è¡Œä¸€å®šçš„è½¬æ¢ã€‚
## ELMo embeddings 
elmo vector: å¯¹äºæ¯ä¸ªå•è¯éƒ½æœ‰ä¸€ä¸ª ELMo vectorï¼Œå…¶ä¸­æœ‰äº”ä¸ªå‘é‡ï¼Œ
![[Pasted image 20231229171031.png]]
ä¸ºäº†ä¾¿äºè¡¨ç¤ºï¼Œç”¨ä¸‹é¢ä¸‰ä¸ªç¬¦å·ä»£æ›¿
$$
h_t^1 = \{h_t^{1l}, h_t^{1r}\} 
$$
$$

h_t^2 = \{h_t^{2l}, h_t^{2r}\}

$$
$$
h_t^0 = x_t
$$

### ELMo embeddings training
![[Pasted image 20231229171833.png]]
ä¸‹é¢è¿™äº›éƒ½è¦è®­ç»ƒ
![[Pasted image 20231229172001.png]]

## how we use them

We use the embeddings as our sauce. 
å°† ELMo embeddings ä¸æˆ‘ä»¬æƒ³è¦çš„embedding ç›®æ ‡è¿æ¥èµ·æ¥

![[Pasted image 20231229174616.png]]
### Secret Sauce
 ELMo embedding give us the flexibility to fine-tune and adjust the embeddings *for our specific needs*.
![[Pasted image 20231229180100.png]]

The scalar ğœ¸ and the vector ğ’” are trained by the model. The value of ğ’”  is softmax normalized.

ğœ¸ å’Œ ğ’” éƒ½å¯ä»¥æ ¹æ®æˆ‘ä»¬çš„éœ€è¦æ¥è¿›è¡Œè®¾ç½®è°ƒæ•´ï¼Œæ¯”å¦‚å¯¹äºä½å­—ç¬¦çº§åµŒå…¥æ”¶ç›Šæ›´é«˜çš„æ¨¡å‹ï¼Œå¯ä»¥è®¾ç½®[1,0,0]çš„æ¨¡å‹ï¼Œå…·ä½“å¦‚ä¸‹ï¼š
![[Pasted image 20231229180314.png]]
å½“ç„¶ä¹Ÿå¯ä»¥è®¾ç½®ä¸åŒçš„æ¯”ä¾‹ï¼Œæ¥åšæ›´åŠ ç²¾ç¡®çš„æ§åˆ¶