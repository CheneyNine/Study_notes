# 什么是深度学习

深度：网络层数多
核心问题：解决多层网络可以训练进而演化而来的一套学习方法

# 深度学习中的两大问题

1. 梯度消失问题：误差在反向传播过程中逐渐衰减
2. 过拟合问题：神经网络将训练样本特有的噪声也误认为是类别特征，泛化能力变弱
（ link 现在需要的是特征式学习，而不是模板式学习）

# 解决方法
## 解决梯度消失的问题

采用合适的激活函数：ReLU
	激活函数在求偏导函数下的性能：relu 偏导恒为 1，可以维持 Err 的传播
批规范化方法：
	将每个隐藏输入的分布强行拉回到均值为 0 方差为 1 的标准正态分布上->让输入值落回到激活函数的敏感区域（sigmoid 的函数的中间，在两段则会一致保持在 0 和 1 附近）
	通过归一化每个小批量的输出，批量归一化可以帮助减少内部协变量偏移，从而缓解梯度消失的问题
	link（粒子群算法：扔进去一个新的个体去改变当前的状态）
	[[2. 智能计算#群体智能——粒子群优化]]
采用特殊的网络结构解决
	残差网络：最开始的输出层可以跳过中间的部分层
	[[LSTM]]网络

## 解决过拟合问题
欠拟合、合适、过拟合的概念
解决方法：
1. 🌟获取更多数据：数据增强，扩充数据量
2. 数据清洗
3. 减少参数：效果不好
4. 🌟Dropout：减少全连接，效果很好
5. 🌟正则化：在训练的时候限制权值变大
6. 限制训练时间：不是我们所追求的

# 其他深度学习模型

DBN

RNN

LSTM

DBN 

GAN：一个网络用于生成一个用于检测？ 最后输出样本是真是样本而不是对抗样本的概率。
能够很好地解决生成的问题。支持生成式学习。（生成式学习和判别式学习）

CapsulesNetwork(和如今的多智能体网络结果一样，一个小算子解决一个问题，和 Agent 一样)

# 生成对抗网络在图像处理中的应用

风格迁移（内容图像+风格图像）：在风格特征尤为明显的情况下，很容易被深度学习感知，但是深度学习很难感受到小差异。

识别、分类等等
