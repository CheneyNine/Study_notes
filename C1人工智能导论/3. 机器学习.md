

# 概要

>[!note] 考察 
>概要部分稍微了解，做判断

原先智能计算的参数设置、自适应调整方式仍需要人工来设计。因此，想借助机器来实现*自动化的参数调整*。

## 相关学科


## 应用场景


## 方法论

TAEP 
任务 方法 经验 性能
研究对象 核心内容 动力源泉

任务-T：机器学习要解决的问题
方法-A：各种机器学习方法
经验-E：训练模型的数据，实例
性能-P：方法针对任务的性能评估准则
任务是机器学习的研究对象
方法是机器学习的核心内容
经验是机器学习的动力源泉
性能是机器学习的检验指标




# 分类

监督学习

无监督学习

半监督学习：数据没有标签，机器学习出的模型是从数据中提取出来的模式，一般是分类或回归等任务

强化学习：外部环境对输入只给出评价信息而非正确答案，学习机通过强化受奖励的动作来改善自身的性能，常见应用场景是动态系统、机器人控制

# 评价
误差
误差率
。。。


# 监督学习
## KNN
核心逻辑：用距离评价相似性+用投票进行学习
直接反映了数据集本身的分布逻辑
是最原始的学习方式


数据预处理

度量距离：曼哈顿距离、欧式距离

特征属性归一化的必要性：

>[!note] 考察
>课堂练习的题目->考试题目


### 优点 
1）算法简单，理论成熟，既可以用来做分类也可以用来做回归。
2）可用于*非线性分类*。
3）没有明显的训练过程，而是在程序开始运行时，把数据集加载到内存后，不需要进行训练，直接进行预测，所以训练*时间复杂度为0*。
4）由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属的类别，因此对于*类域的交叉或重叠较多的待分类样本集*来说，KNN方法较其他方法更为适合。
5）该算法比较适用于*样本容量比较大的类域的自动分类*，而那些样本容量比较小的类域采用这种算法比较容易产生误分类情况。


### 缺点
1）需要算每个测试点与训练集的距离，当训练集较大时，*计算量相当大，时间复杂度高*，特别是特征数量比较大的时候。
2）需要*大量的内存，空间复杂度高*。
3）*样本不平衡*问题（即有些类别的样本数量很多，而其它样本的数量很少），对稀有类别的预测准确度低。
4）是lazy learning方法，*基本上不学习*，导致预测时速度比起逻辑回归之类的算法慢。

### 思考
1. **样本不平衡**：样本不平衡指的是不同类别的样本数量差异很大。这在KNN中可能导致算法偏向于样本数较多的类别。为了解决这个问题，可以采取以下措施：
    
    - 重采样：通过过采样少数类或欠采样多数类来平衡数据。
    - 调整权重：给少数类的样本赋予更高的权重。
2. **降低超参数K的影响**：K是KNN算法中唯一的重要超参数，它决定了用于做出预测的邻居的数量。选择合适的K值是至关重要的，可以通过以下方法来选择：
    
    - 交叉验证：使用交叉验证来找到最佳的K值。
    - 距离加权：对邻居进行加权，距离越近的邻居权重越大，可以减少K值选择不当带来的影响。
3. **分类问题中距离的本质**：在KNN中，距离用于衡量样本之间的*相似度*。常用的距离度量包括欧几里得距离、曼哈顿距离和余弦相似度。选择哪种距离度量取决于*数据的特性和问题的性质*。
    
4. **符号或布尔值的距离度量**：对于符号性或布尔值数据，可以使用如*汉明距离等特别的度量方法*，这种距离度量基于不同符号或布尔值之间的差异。
    
5. **加快运算速度**：KNN算法在大数据集上可能非常慢，因为它需要计算每个查询点与所有训练样本之间的距离。加快计算速度的方法包括：
    
    - 使用近似最近邻搜索算法，如KD树或球树。
    - 降维：使用PCA或其他降维技术减少数据的维度。
    - 并行处理：利用多核处理器进行并行计算。

## 决策树概述

目标 
分类

## 信息熵

很好地表达系统的不确定性

能够减小信息熵的特征就是好的特征

![image-20231129104331904](image/image-20231129104331904.png)



## 决策树ID3

核心：从高价值特征入手，对数据进行分类，以信息增益来度量属性的选择，对“特征”进行指标化评价

### 优点

只需对训练实例进行较好的标注就可以学习

分类模型是树状结构，简单直观

### 过程

先根据结论计算信息熵A1

依次选择各个特征计算划分后的信息熵B1

计算信息增益（用信息熵 B进行加权计算后与 A 进行差值计算）看信息熵变少了多少

选择信息增益最大的作为第一节点

基于第一节点的分类，进行筛选，再循环计算分类后的整体信息熵 A2、划分后的信息熵 B2...

![image-20231129105323961](image/image-20231129105323961.png)

### 弊端

决策树偏爱类别比较多的特征

-任何特征的取值一但很多，因为分类很多，不管子类中有多混沌，降熵效果都会很好。

无法处理连续值属性、无法处理属性值不完整的训练数据

可能会受噪声或小样本的影响，出现过拟合问题、不稳定性：
	•	决策树对数据中的小变化可能非常敏感，导致最终模型的大幅度改变。
	•	解决方法：使用集成方法，如**随机森林**（重复选几次训练集合，生成多个树形成随机森林），来增加稳定性。

## 决策树 C4.5

### 过拟合问题

1. 预剪枝

   在生成的同时进行剪枝，设定阈值，大于阈值才分类

2. 后剪枝

随机森林

## 回归

>[!note] 考察
概念基本了解就行
最小二乘法的线性回归

## 回归与分类

本质上区别是连续和离散

回归分析离散化可以做分类，分类模型连续化可以做回归，但一般不这么做

## 线性回归

序列数据直接的关系

统计分析-寻找映射关系

逻辑：最小二乘法

# 无监督学习

本质上是在训练的时候有通用标签

自我归纳、挖掘内在规律

## K均值聚类（K-means）

>[!note] 考察
>不考计算题，侧重计算逻辑

体现数据最本质特征-距离

背后是在关注数据之间的相似还是不相似

数据之间距离的表示-欧氏距离

评价好坏：方差（类内高内聚）

### 步骤

1. 选取 K 个对象作为分类中心
2. 根据最小距离分配
3. 计算新的分类中心，更改分类中心
4. 迭代直到方差几乎不变

### 小结

有时候不知道 K 多少合适？可以多次尝试，观察最后的方差哪个好

起始中心的影响？用方差或别的评价模型评价好坏

## 人工神经网络

深度学习的前身

## 概述

核心是生物自然社会规律的算法化

人工神经网络的特性：并行分布处理、非线性映射、通过训练学习、适应与集成、硬件实现

## 发展历史

1. **萌芽期** 

   ​	在这一时期，计算机科学和人工智能的基础概念开始形成。

2. **第一次高潮期**

   - 冯·诺伊曼奠定了分布式计算的基础，为后续的AI发展提供了重要的计算平台。
   - 感知机模型的提出（由罗森布拉特等人），标志着早期神经网络的起步。

3. **反思期**

   - Minsky和Papert对感知机的原理进行了深入分析，指出了其局限性，这导致了对神经网络研究的怀疑和资金支持的减少。

4. **第二次高潮期**

   - Hopfield提出了Hopfield网络，引入了能量函数概念，这对于神经网络的稳定性和记忆功能有重要影响。
   - Rumelhart和Hinton提出反向传播算法，解决了多层前向神经网络的学习问题，这一发现极大推动了神经网络研究的发展。

5. **再认识与应用研究**

   - 在这一时期，研究者开始重新审视神经网络模型，并根据新的理论和技术对现有模型进行修改和完善。
   - 应用研究开始兴起，神经网络被应用于各种实际问题中。

6. **深度神经网络时期**

   - 2006年，Hinton等人提出了深度学习的概念，引入深度神经网络，这标志着人工智能进入一个新的时代。逐层初始化，克服训练难度。

   - 深度学习在图像识别、语音识别和其他领域取得了显著成就，极大地推动了人工智能技术的发展。
   

## 第一个神经元模型

1943 年，提出的 MP 模型，这是一个简单的二元分类器（输出 0 或1）

缺点：不能学习、不能自动调整，输入输出都是二元的

本质上只是一种逻辑门，有一定的计算能力

## 赫布学习模型

同时被激活的神经元之间的连接应该被强化

前面激活后面激活就加强权重

## 感知机（perceptron）

>[!note] 考察
>动手实践，将 3-4 个点分开的计算过程


改进点：

1. 输入可以是实数向量
2. 有多种激活函数（在事先定好的范围内输出，对输入进去的数据处理，改变其分布）可以选择
3. 属于是一个可学习模型

学习算法：

距离量化、误分类点距离、计算损失函数：距离的总和

调整算法：

求解最优化问题

随机梯度下降法



---
缺课
单层感知机不能解决异或问题
多层感知机可以，但是很难训练，超参数的设置

---

## BP 反向传播算法

>[!note] 考察
>纯考概念 选择（不同阶段选择什么对应的公式），不考察公式和计算

反向传播函数

> 如何设计网络拓扑结构？
>   输入单元数、隐藏层数、每个隐藏层的单元数和输出层的单元数

### 如何后向传播

后向传播迭代地处理训练数据集，将每个元祖的网络预测与实际已知目标值比较。

通过梯度知道向哪个方向传播


计算步骤
1. 计算输出层的输出与实际输出的误差，将误差代入相关数学公式计算 Err
2. 根据学习率、输入和 Err 去更新相关的 w θ 等参数（计算 w需要输入，计算 θ 不需要）
3. 隐藏层中，计算公式一致，计算项发生变化，Err 从输出从中加权求和获取

![[Pasted image 20231213105456.png]]

### BP 算法小结
1. 核心思想：利用前向传播，计算第 n 层的输出值
2.  优化目标：输出值和实际值的残差
3. 计算方法：按残差反向回传（误差逆传播）
4. 主要工具：链式法则
### 局限性
1. 容易过拟合-> 早停、正则化（训练到一定程度，输入的影响越来越小，则重新在隐藏层和输出层中间做正则，把输入拉开）
2. 容易陷入局部最优->选取多次初值、随机梯度下降法
3. 难以设置隐藏层个数->试错法
4. ❗️最大的问题：误差传不回去，不同层之间的 Err 数量级差异过大


