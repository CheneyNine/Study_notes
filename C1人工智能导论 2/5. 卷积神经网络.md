# 发展历史
- 1962 年 Hubel&Wiesel 提出感受野（receptive field）的概念
- 1984 年 提出神经认知机（neocongintion）
- 1989 年 CNN 的现代结构，后完善，取名 LeNet-5
- 2006 年 经典深CNN—AlexNet
- 2014 年 ZFNet, VGGNet, 残差网络ResNet

# 简介

## CNN 的一般结构
输入层

卷积层：提取不同层次的不同特征，
一个卷积核的处理代表了图片的一个侧面特征

池化层：向下采样，把分辨率较高的图片转换为分辨率较低的图片，减少参数，实现非线性，扩大感受野，赋予不变性

>[!note]
>感受野：看一个像素就像看了 9 个像素
>不变性：像素有一点抖动、移位，但因为一个像素可以代表 4 个像素，所以这种一点偏差能被吸收

>[!note] 卷积+最大池化
>本质上是给图片降维

全连接层（隐藏层）

输出层：使用 Softmax 实现多分类或 sigmoid 实现二分类，将网络层提取到的特征转换为我们需要输出的形式。
## CNN 核心思想【选择/判断】

1. 局部感受野，类似于[[4. 深度学习]]中的 Dropout，可以减少很多参数。解释逻辑：人眼看图片的时候更多关注的是局部。
2. 权值共享，本质上也是为了减少一些训练过程中的参数，整个模型的关键在于卷积，而不是全连接
3. 时间或空间亚采样

## CNN 详解 

### 卷积

卷积核背后：算子
高频算子、拉普拉斯算子....

通过卷积可以获得图像中的关键特征，经过特定卷积矩阵滤波后，所得到结果可以视为是保留了像素点所构成的特定空间的分布模式

>[!example] 高斯卷积核
>对像素差异很敏感？

### 池化
目的也是为了降低分辨率，利用局部信息来进行下采样，减少运算量，保留有用特征
目的就是降分辨率，不是提取特征

>[!example] 池化方式
>最大池化、平均池化、Lp池化（计算范数）、混合池化

1. 减少参数和计算量，防止过拟合
2. 是模型对尺度平移、旋转变化具有一定的不变形

### 激励函数

ReLu：恒正，可以很好的保留特征

此外还有其他一些函数，对负数稍微有一定的敏感性。
- LReLu
- RReLu
- ELU
- ...

### 全连接与分类层

全连接层的输入都是向量

经过卷积后的特征图可以经过多层卷积和池化后最终得到很多小特征图，当这些结果小到一定程度的时候就可以作为一个向量输入





## 经典模型

### LeNet(1998)

### AlexNet(2012)

### VGNet(2014)
目标：分类、物体检测
### GoogleNet(2014)
特点：并行逻辑、不同大小的卷积核存在

用不同大小的卷积核提取不同特征-->多尺度概念

### ResNet(残差神经网络)
印证了：神经网络越深越好的猜想
解决梯度消失与爆炸：调接思想

